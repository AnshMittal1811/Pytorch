{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03_Transforms_in_PyTorch.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNNC4/RK1ndCnUhE/GDQrxF"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4a04c95a8db046e7acc0599ebc311224": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_9ee2f00a2db44838910bcb880977afdc",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_8e3646bf39994f3fbf2f2571e571eacf",
              "IPY_MODEL_25ad4d342f17446fa2873808582cc197",
              "IPY_MODEL_2cc7a59aa3824ba68ccae467ea767426"
            ]
          }
        },
        "9ee2f00a2db44838910bcb880977afdc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8e3646bf39994f3fbf2f2571e571eacf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_cceec4a6d18846a795d92c8259bb1521",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_323439f183c94005bb2ec01c3d58d7a9"
          }
        },
        "25ad4d342f17446fa2873808582cc197": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e8b0170713434829ac386f10f96099b5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 26421880,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 26421880,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_231fef5f34b74308a0018cf05cf7c0eb"
          }
        },
        "2cc7a59aa3824ba68ccae467ea767426": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_26230b60cd21495cad845950e8d4e124",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 26422272/? [00:01&lt;00:00, 27023720.19it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2c8b9936868848bda67a8f994fa54574"
          }
        },
        "cceec4a6d18846a795d92c8259bb1521": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "323439f183c94005bb2ec01c3d58d7a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e8b0170713434829ac386f10f96099b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "231fef5f34b74308a0018cf05cf7c0eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "26230b60cd21495cad845950e8d4e124": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2c8b9936868848bda67a8f994fa54574": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8e8f5c15961f424ba0fe8688c1a76bdf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_345e50855ff7415f88e3a46326bd3cad",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_9769908ec599472dbdb5171f481cd4f6",
              "IPY_MODEL_560c82fb2d974d509095ead258a9fca1",
              "IPY_MODEL_4b38bfc4fef2459890c2ac47d285f2b8"
            ]
          }
        },
        "345e50855ff7415f88e3a46326bd3cad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9769908ec599472dbdb5171f481cd4f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_060396e434334403b349e25d6e6f009c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_998783d856ee404c82f7ea2cab697a61"
          }
        },
        "560c82fb2d974d509095ead258a9fca1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_52ddd03cb35c4740ac7587eae9a02130",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 29515,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 29515,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_abfe3975244145d7853f9048adcbd313"
          }
        },
        "4b38bfc4fef2459890c2ac47d285f2b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1f5542c79e8040e1ae2836736d67c4c7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 29696/? [00:00&lt;00:00, 299952.44it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7bda7bab236d45c7a7a209fb048949ac"
          }
        },
        "060396e434334403b349e25d6e6f009c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "998783d856ee404c82f7ea2cab697a61": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "52ddd03cb35c4740ac7587eae9a02130": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "abfe3975244145d7853f9048adcbd313": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1f5542c79e8040e1ae2836736d67c4c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7bda7bab236d45c7a7a209fb048949ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f66c350e3b6b46cda5800523d4a1d923": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e13af968c3ee45d892e0e46a5c1c08cd",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_bb03edffa7d54f499e6bbc55accc27f0",
              "IPY_MODEL_b49077147a2c4b078bfac5f8b70e7c64",
              "IPY_MODEL_4c567dfc039b461587e12f2e1443d3b4"
            ]
          }
        },
        "e13af968c3ee45d892e0e46a5c1c08cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bb03edffa7d54f499e6bbc55accc27f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c2b38528b3f442789573fceeff26f798",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6974bec03b654adf83756539bf7ea3fe"
          }
        },
        "b49077147a2c4b078bfac5f8b70e7c64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ea7eaa7253a14017af3b962835d10219",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 4422102,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 4422102,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e2018e152766464eb55d57dd29ecebb2"
          }
        },
        "4c567dfc039b461587e12f2e1443d3b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2e7edb77fd8b4f99a1faa4b417d7fb81",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 4422656/? [00:00&lt;00:00, 10093127.17it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e38fb540792848d7a2fdf927dd4a1c13"
          }
        },
        "c2b38528b3f442789573fceeff26f798": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6974bec03b654adf83756539bf7ea3fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ea7eaa7253a14017af3b962835d10219": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e2018e152766464eb55d57dd29ecebb2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2e7edb77fd8b4f99a1faa4b417d7fb81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e38fb540792848d7a2fdf927dd4a1c13": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4ecb2541353248d4918588d8f99aa797": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c5129e96f9ad4bcca376e976050d3e4b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_cc7f7742976e42989c2e749de2297dc1",
              "IPY_MODEL_9bcf8ca0710849859ed71e44c658752f",
              "IPY_MODEL_af5529effa7d4a838c44d3b42e520d2a"
            ]
          }
        },
        "c5129e96f9ad4bcca376e976050d3e4b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cc7f7742976e42989c2e749de2297dc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4d6a9d154fbd4b90bddc3d280bd706fe",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a063517faf9a4451b94b16660c1528e3"
          }
        },
        "9bcf8ca0710849859ed71e44c658752f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_4620d4a516dd4f07b5a836e2676fcb7e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 5148,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 5148,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5c2f63f8073048ac93886276f6ef7590"
          }
        },
        "af5529effa7d4a838c44d3b42e520d2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_71854e7f40074d9baae5ec79a8b6786a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 6144/? [00:00&lt;00:00, 124082.39it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_27758539240443dcbe5780a5a28ad862"
          }
        },
        "4d6a9d154fbd4b90bddc3d280bd706fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a063517faf9a4451b94b16660c1528e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4620d4a516dd4f07b5a836e2676fcb7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5c2f63f8073048ac93886276f6ef7590": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "71854e7f40074d9baae5ec79a8b6786a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "27758539240443dcbe5780a5a28ad862": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435,
          "referenced_widgets": [
            "4a04c95a8db046e7acc0599ebc311224",
            "9ee2f00a2db44838910bcb880977afdc",
            "8e3646bf39994f3fbf2f2571e571eacf",
            "25ad4d342f17446fa2873808582cc197",
            "2cc7a59aa3824ba68ccae467ea767426",
            "cceec4a6d18846a795d92c8259bb1521",
            "323439f183c94005bb2ec01c3d58d7a9",
            "e8b0170713434829ac386f10f96099b5",
            "231fef5f34b74308a0018cf05cf7c0eb",
            "26230b60cd21495cad845950e8d4e124",
            "2c8b9936868848bda67a8f994fa54574",
            "8e8f5c15961f424ba0fe8688c1a76bdf",
            "345e50855ff7415f88e3a46326bd3cad",
            "9769908ec599472dbdb5171f481cd4f6",
            "560c82fb2d974d509095ead258a9fca1",
            "4b38bfc4fef2459890c2ac47d285f2b8",
            "060396e434334403b349e25d6e6f009c",
            "998783d856ee404c82f7ea2cab697a61",
            "52ddd03cb35c4740ac7587eae9a02130",
            "abfe3975244145d7853f9048adcbd313",
            "1f5542c79e8040e1ae2836736d67c4c7",
            "7bda7bab236d45c7a7a209fb048949ac",
            "f66c350e3b6b46cda5800523d4a1d923",
            "e13af968c3ee45d892e0e46a5c1c08cd",
            "bb03edffa7d54f499e6bbc55accc27f0",
            "b49077147a2c4b078bfac5f8b70e7c64",
            "4c567dfc039b461587e12f2e1443d3b4",
            "c2b38528b3f442789573fceeff26f798",
            "6974bec03b654adf83756539bf7ea3fe",
            "ea7eaa7253a14017af3b962835d10219",
            "e2018e152766464eb55d57dd29ecebb2",
            "2e7edb77fd8b4f99a1faa4b417d7fb81",
            "e38fb540792848d7a2fdf927dd4a1c13",
            "4ecb2541353248d4918588d8f99aa797",
            "c5129e96f9ad4bcca376e976050d3e4b",
            "cc7f7742976e42989c2e749de2297dc1",
            "9bcf8ca0710849859ed71e44c658752f",
            "af5529effa7d4a838c44d3b42e520d2a",
            "4d6a9d154fbd4b90bddc3d280bd706fe",
            "a063517faf9a4451b94b16660c1528e3",
            "4620d4a516dd4f07b5a836e2676fcb7e",
            "5c2f63f8073048ac93886276f6ef7590",
            "71854e7f40074d9baae5ec79a8b6786a",
            "27758539240443dcbe5780a5a28ad862"
          ]
        },
        "id": "xDZO-20kmEv8",
        "outputId": "5483b09f-500d-4be4-be67-36ca167ab256"
      },
      "source": [
        "import torch\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor, Lambda\n",
        "\n",
        "data = datasets.FashionMNIST(root = \"data\",\n",
        "                             train = True,\n",
        "                             download = True,\n",
        "                             transform = ToTensor(),\n",
        "                             target_transform = Lambda(lambda y: torchzeros(10, dtype = torch.float).scatter_(0, torch.tensor(y), value = 1)))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4a04c95a8db046e7acc0599ebc311224",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/26421880 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8e8f5c15961f424ba0fe8688c1a76bdf",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/29515 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f66c350e3b6b46cda5800523d4a1d923",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/4422102 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4ecb2541353248d4918588d8f99aa797",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/5148 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSBhtKCTnXwt"
      },
      "source": [
        "## ToTensor()\n",
        "This converts a PIL image or Numpy Array into FloatTensor and then scales all these values (for e.g., from 255) to values between [0., 1.]. \n",
        "## Lambda Transforms\n",
        "Lambda transforms apply any user-defined lambda functions. Define a function to turn the imteger into a one-hot encoded tensor.\n",
        "1. Creates a 0-tensor of size 10 (number of labels);\n",
        "2. scatters using ```scatter_``` which assigns a ```value = 1``` on index as given by label ```y```."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-eQLy1qmrKq"
      },
      "source": [
        "target_transform = Lambda(lambda y: torch.zeros(10, dtype = torch.float).\n",
        "                          scatter_(dim = 0, index = torch.tensor(y), value = 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJNqLiFDp7ji"
      },
      "source": [
        "## Source Code for all transforms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "id": "BjpkfgyEnJ8x",
        "outputId": "f1a563c0-077c-43dc-a65b-7b1ce5f4f543"
      },
      "source": [
        "import math\n",
        "import numbers\n",
        "import random\n",
        "import warnings\n",
        "from collections.abc import Sequence\n",
        "from typing import Tuple, List, Optional\n",
        "\n",
        "import torch\n",
        "from torch import Tensor\n",
        "\n",
        "try:\n",
        "    import accimage\n",
        "except ImportError:\n",
        "    accimage = None\n",
        "\n",
        "from . import functional as F\n",
        "from .functional import InterpolationMode, _interpolation_modes_from_int\n",
        "\n",
        "\n",
        "__all__ = [\"Compose\", \"ToTensor\", \"PILToTensor\", \"ConvertImageDtype\", \"ToPILImage\", \"Normalize\", \"Resize\", \"Scale\",\n",
        "           \"CenterCrop\", \"Pad\", \"Lambda\", \"RandomApply\", \"RandomChoice\", \"RandomOrder\", \"RandomCrop\",\n",
        "           \"RandomHorizontalFlip\", \"RandomVerticalFlip\", \"RandomResizedCrop\", \"RandomSizedCrop\", \"FiveCrop\", \"TenCrop\",\n",
        "           \"LinearTransformation\", \"ColorJitter\", \"RandomRotation\", \"RandomAffine\", \"Grayscale\", \"RandomGrayscale\",\n",
        "           \"RandomPerspective\", \"RandomErasing\", \"GaussianBlur\", \"InterpolationMode\", \"RandomInvert\", \"RandomPosterize\",\n",
        "           \"RandomSolarize\", \"RandomAdjustSharpness\", \"RandomAutocontrast\", \"RandomEqualize\"]\n",
        "\n",
        "\n",
        "class Compose:\n",
        "    \"\"\"Composes several transforms together. This transform does not support torchscript.\n",
        "    Please, see the note below.\n",
        "\n",
        "    Args:\n",
        "        transforms (list of ``Transform`` objects): list of transforms to compose.\n",
        "\n",
        "    Example:\n",
        "        >>> transforms.Compose([\n",
        "        >>>     transforms.CenterCrop(10),\n",
        "        >>>     transforms.PILToTensor(),\n",
        "        >>>     transforms.ConvertImageDtype(torch.float),\n",
        "        >>> ])\n",
        "\n",
        "    .. note::\n",
        "        In order to script the transformations, please use ``torch.nn.Sequential`` as below.\n",
        "\n",
        "        >>> transforms = torch.nn.Sequential(\n",
        "        >>>     transforms.CenterCrop(10),\n",
        "        >>>     transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
        "        >>> )\n",
        "        >>> scripted_transforms = torch.jit.script(transforms)\n",
        "\n",
        "        Make sure to use only scriptable transformations, i.e. that work with ``torch.Tensor``, does not require\n",
        "        `lambda` functions or ``PIL.Image``.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, transforms):\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __call__(self, img):\n",
        "        for t in self.transforms:\n",
        "            img = t(img)\n",
        "        return img\n",
        "\n",
        "    def __repr__(self):\n",
        "        format_string = self.__class__.__name__ + '('\n",
        "        for t in self.transforms:\n",
        "            format_string += '\\n'\n",
        "            format_string += '    {0}'.format(t)\n",
        "        format_string += '\\n)'\n",
        "        return format_string\n",
        "\n",
        "\n",
        "\n",
        "class ToTensor:\n",
        "    \"\"\"Convert a ``PIL Image`` or ``numpy.ndarray`` to tensor. This transform does not support torchscript.\n",
        "\n",
        "    Converts a PIL Image or numpy.ndarray (H x W x C) in the range\n",
        "    [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0]\n",
        "    if the PIL Image belongs to one of the modes (L, LA, P, I, F, RGB, YCbCr, RGBA, CMYK, 1)\n",
        "    or if the numpy.ndarray has dtype = np.uint8\n",
        "\n",
        "    In the other cases, tensors are returned without scaling.\n",
        "\n",
        "    .. note::\n",
        "        Because the input image is scaled to [0.0, 1.0], this transformation should not be used when\n",
        "        transforming target image masks. See the `references`_ for implementing the transforms for image masks.\n",
        "\n",
        "    .. _references: https://github.com/pytorch/vision/tree/main/references/segmentation\n",
        "    \"\"\"\n",
        "\n",
        "    def __call__(self, pic):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Converted image.\n",
        "        \"\"\"\n",
        "        return F.to_tensor(pic)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '()'\n",
        "\n",
        "\n",
        "\n",
        "class PILToTensor:\n",
        "    \"\"\"Convert a ``PIL Image`` to a tensor of the same type. This transform does not support torchscript.\n",
        "\n",
        "    Converts a PIL Image (H x W x C) to a Tensor of shape (C x H x W).\n",
        "    \"\"\"\n",
        "\n",
        "    def __call__(self, pic):\n",
        "        \"\"\"\n",
        "        .. note::\n",
        "\n",
        "            A deep copy of the underlying array is performed.\n",
        "\n",
        "        Args:\n",
        "            pic (PIL Image): Image to be converted to tensor.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Converted image.\n",
        "        \"\"\"\n",
        "        return F.pil_to_tensor(pic)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '()'\n",
        "\n",
        "\n",
        "\n",
        "class ConvertImageDtype(torch.nn.Module):\n",
        "    \"\"\"Convert a tensor image to the given ``dtype`` and scale the values accordingly\n",
        "    This function does not support PIL Image.\n",
        "\n",
        "    Args:\n",
        "        dtype (torch.dtype): Desired data type of the output\n",
        "\n",
        "    .. note::\n",
        "\n",
        "        When converting from a smaller to a larger integer ``dtype`` the maximum values are **not** mapped exactly.\n",
        "        If converted back and forth, this mismatch has no effect.\n",
        "\n",
        "    Raises:\n",
        "        RuntimeError: When trying to cast :class:`torch.float32` to :class:`torch.int32` or :class:`torch.int64` as\n",
        "            well as for trying to cast :class:`torch.float64` to :class:`torch.int64`. These conversions might lead to\n",
        "            overflow errors since the floating point ``dtype`` cannot store consecutive integers over the whole range\n",
        "            of the integer ``dtype``.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dtype: torch.dtype) -> None:\n",
        "        super().__init__()\n",
        "        self.dtype = dtype\n",
        "\n",
        "    def forward(self, image):\n",
        "        return F.convert_image_dtype(image, self.dtype)\n",
        "\n",
        "\n",
        "\n",
        "class ToPILImage:\n",
        "    \"\"\"Convert a tensor or an ndarray to PIL Image. This transform does not support torchscript.\n",
        "\n",
        "    Converts a torch.*Tensor of shape C x H x W or a numpy ndarray of shape\n",
        "    H x W x C to a PIL Image while preserving the value range.\n",
        "\n",
        "    Args:\n",
        "        mode (`PIL.Image mode`_): color space and pixel depth of input data (optional).\n",
        "            If ``mode`` is ``None`` (default) there are some assumptions made about the input data:\n",
        "            - If the input has 4 channels, the ``mode`` is assumed to be ``RGBA``.\n",
        "            - If the input has 3 channels, the ``mode`` is assumed to be ``RGB``.\n",
        "            - If the input has 2 channels, the ``mode`` is assumed to be ``LA``.\n",
        "            - If the input has 1 channel, the ``mode`` is determined by the data type (i.e ``int``, ``float``,\n",
        "            ``short``).\n",
        "\n",
        "    .. _PIL.Image mode: https://pillow.readthedocs.io/en/latest/handbook/concepts.html#concept-modes\n",
        "    \"\"\"\n",
        "    def __init__(self, mode=None):\n",
        "        self.mode = mode\n",
        "\n",
        "    def __call__(self, pic):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            pic (Tensor or numpy.ndarray): Image to be converted to PIL Image.\n",
        "\n",
        "        Returns:\n",
        "            PIL Image: Image converted to PIL Image.\n",
        "\n",
        "        \"\"\"\n",
        "        return F.to_pil_image(pic, self.mode)\n",
        "\n",
        "    def __repr__(self):\n",
        "        format_string = self.__class__.__name__ + '('\n",
        "        if self.mode is not None:\n",
        "            format_string += 'mode={0}'.format(self.mode)\n",
        "        format_string += ')'\n",
        "        return format_string\n",
        "\n",
        "\n",
        "\n",
        "class Normalize(torch.nn.Module):\n",
        "    \"\"\"Normalize a tensor image with mean and standard deviation.\n",
        "    This transform does not support PIL Image.\n",
        "    Given mean: ``(mean[1],...,mean[n])`` and std: ``(std[1],..,std[n])`` for ``n``\n",
        "    channels, this transform will normalize each channel of the input\n",
        "    ``torch.*Tensor`` i.e.,\n",
        "    ``output[channel] = (input[channel] - mean[channel]) / std[channel]``\n",
        "\n",
        "    .. note::\n",
        "        This transform acts out of place, i.e., it does not mutate the input tensor.\n",
        "\n",
        "    Args:\n",
        "        mean (sequence): Sequence of means for each channel.\n",
        "        std (sequence): Sequence of standard deviations for each channel.\n",
        "        inplace(bool,optional): Bool to make this operation in-place.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, mean, std, inplace=False):\n",
        "        super().__init__()\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "        self.inplace = inplace\n",
        "\n",
        "    def forward(self, tensor: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            tensor (Tensor): Tensor image to be normalized.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Normalized Tensor image.\n",
        "        \"\"\"\n",
        "        return F.normalize(tensor, self.mean, self.std, self.inplace)\n",
        "\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)\n",
        "\n",
        "\n",
        "\n",
        "class Resize(torch.nn.Module):\n",
        "    \"\"\"Resize the input image to the given size.\n",
        "    If the image is torch Tensor, it is expected\n",
        "    to have [..., H, W] shape, where ... means an arbitrary number of leading dimensions\n",
        "\n",
        "    .. warning::\n",
        "        The output image might be different depending on its type: when downsampling, the interpolation of PIL images\n",
        "        and tensors is slightly different, because PIL applies antialiasing. This may lead to significant differences\n",
        "        in the performance of a network. Therefore, it is preferable to train and serve a model with the same input\n",
        "        types. See also below the ``antialias`` parameter, which can help making the output of PIL images and tensors\n",
        "        closer.\n",
        "\n",
        "    Args:\n",
        "        size (sequence or int): Desired output size. If size is a sequence like\n",
        "            (h, w), output size will be matched to this. If size is an int,\n",
        "            smaller edge of the image will be matched to this number.\n",
        "            i.e, if height > width, then image will be rescaled to\n",
        "            (size * height / width, size).\n",
        "\n",
        "            .. note::\n",
        "                In torchscript mode size as single int is not supported, use a sequence of length 1: ``[size, ]``.\n",
        "        interpolation (InterpolationMode): Desired interpolation enum defined by\n",
        "            :class:`torchvision.transforms.InterpolationMode`. Default is ``InterpolationMode.BILINEAR``.\n",
        "            If input is Tensor, only ``InterpolationMode.NEAREST``, ``InterpolationMode.BILINEAR`` and\n",
        "            ``InterpolationMode.BICUBIC`` are supported.\n",
        "            For backward compatibility integer values (e.g. ``PIL.Image.NEAREST``) are still acceptable.\n",
        "        max_size (int, optional): The maximum allowed for the longer edge of\n",
        "            the resized image: if the longer edge of the image is greater\n",
        "            than ``max_size`` after being resized according to ``size``, then\n",
        "            the image is resized again so that the longer edge is equal to\n",
        "            ``max_size``. As a result, ``size`` might be overruled, i.e the\n",
        "            smaller edge may be shorter than ``size``. This is only supported\n",
        "            if ``size`` is an int (or a sequence of length 1 in torchscript\n",
        "            mode).\n",
        "        antialias (bool, optional): antialias flag. If ``img`` is PIL Image, the flag is ignored and anti-alias\n",
        "            is always used. If ``img`` is Tensor, the flag is False by default and can be set to True for\n",
        "            ``InterpolationMode.BILINEAR`` only mode. This can help making the output for PIL images and tensors\n",
        "            closer.\n",
        "\n",
        "            .. warning::\n",
        "                There is no autodiff support for ``antialias=True`` option with input ``img`` as Tensor.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, size, interpolation=InterpolationMode.BILINEAR, max_size=None, antialias=None):\n",
        "        super().__init__()\n",
        "        if not isinstance(size, (int, Sequence)):\n",
        "            raise TypeError(\"Size should be int or sequence. Got {}\".format(type(size)))\n",
        "        if isinstance(size, Sequence) and len(size) not in (1, 2):\n",
        "            raise ValueError(\"If size is a sequence, it should have 1 or 2 values\")\n",
        "        self.size = size\n",
        "        self.max_size = max_size\n",
        "\n",
        "        # Backward compatibility with integer value\n",
        "        if isinstance(interpolation, int):\n",
        "            warnings.warn(\n",
        "                \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
        "                \"Please, use InterpolationMode enum.\"\n",
        "            )\n",
        "            interpolation = _interpolation_modes_from_int(interpolation)\n",
        "\n",
        "        self.interpolation = interpolation\n",
        "        self.antialias = antialias\n",
        "\n",
        "    def forward(self, img):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img (PIL Image or Tensor): Image to be scaled.\n",
        "\n",
        "        Returns:\n",
        "            PIL Image or Tensor: Rescaled image.\n",
        "        \"\"\"\n",
        "        return F.resize(img, self.size, self.interpolation, self.max_size, self.antialias)\n",
        "\n",
        "\n",
        "    def __repr__(self):\n",
        "        interpolate_str = self.interpolation.value\n",
        "        return self.__class__.__name__ + '(size={0}, interpolation={1}, max_size={2}, antialias={3})'.format(\n",
        "            self.size, interpolate_str, self.max_size, self.antialias)\n",
        "\n",
        "\n",
        "\n",
        "class Scale(Resize):\n",
        "    \"\"\"\n",
        "    Note: This transform is deprecated in favor of Resize.\n",
        "    \"\"\"\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        warnings.warn(\"The use of the transforms.Scale transform is deprecated, \" +\n",
        "                      \"please use transforms.Resize instead.\")\n",
        "        super(Scale, self).__init__(*args, **kwargs)\n",
        "\n",
        "\n",
        "\n",
        "class CenterCrop(torch.nn.Module):\n",
        "    \"\"\"Crops the given image at the center.\n",
        "    If the image is torch Tensor, it is expected\n",
        "    to have [..., H, W] shape, where ... means an arbitrary number of leading dimensions.\n",
        "    If image size is smaller than output size along any edge, image is padded with 0 and then center cropped.\n",
        "\n",
        "    Args:\n",
        "        size (sequence or int): Desired output size of the crop. If size is an\n",
        "            int instead of sequence like (h, w), a square crop (size, size) is\n",
        "            made. If provided a sequence of length 1, it will be interpreted as (size[0], size[0]).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, size):\n",
        "        super().__init__()\n",
        "        self.size = _setup_size(size, error_msg=\"Please provide only two dimensions (h, w) for size.\")\n",
        "\n",
        "    def forward(self, img):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img (PIL Image or Tensor): Image to be cropped.\n",
        "\n",
        "        Returns:\n",
        "            PIL Image or Tensor: Cropped image.\n",
        "        \"\"\"\n",
        "        return F.center_crop(img, self.size)\n",
        "\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '(size={0})'.format(self.size)\n",
        "\n",
        "\n",
        "\n",
        "class Pad(torch.nn.Module):\n",
        "    \"\"\"Pad the given image on all sides with the given \"pad\" value.\n",
        "    If the image is torch Tensor, it is expected\n",
        "    to have [..., H, W] shape, where ... means at most 2 leading dimensions for mode reflect and symmetric,\n",
        "    at most 3 leading dimensions for mode edge,\n",
        "    and an arbitrary number of leading dimensions for mode constant\n",
        "\n",
        "    Args:\n",
        "        padding (int or sequence): Padding on each border. If a single int is provided this\n",
        "            is used to pad all borders. If sequence of length 2 is provided this is the padding\n",
        "            on left/right and top/bottom respectively. If a sequence of length 4 is provided\n",
        "            this is the padding for the left, top, right and bottom borders respectively.\n",
        "\n",
        "            .. note::\n",
        "                In torchscript mode padding as single int is not supported, use a sequence of\n",
        "                length 1: ``[padding, ]``.\n",
        "        fill (number or str or tuple): Pixel fill value for constant fill. Default is 0. If a tuple of\n",
        "            length 3, it is used to fill R, G, B channels respectively.\n",
        "            This value is only used when the padding_mode is constant.\n",
        "            Only number is supported for torch Tensor.\n",
        "            Only int or str or tuple value is supported for PIL Image.\n",
        "        padding_mode (str): Type of padding. Should be: constant, edge, reflect or symmetric.\n",
        "            Default is constant.\n",
        "\n",
        "            - constant: pads with a constant value, this value is specified with fill\n",
        "\n",
        "            - edge: pads with the last value at the edge of the image.\n",
        "              If input a 5D torch Tensor, the last 3 dimensions will be padded instead of the last 2\n",
        "\n",
        "            - reflect: pads with reflection of image without repeating the last value on the edge.\n",
        "              For example, padding [1, 2, 3, 4] with 2 elements on both sides in reflect mode\n",
        "              will result in [3, 2, 1, 2, 3, 4, 3, 2]\n",
        "\n",
        "            - symmetric: pads with reflection of image repeating the last value on the edge.\n",
        "              For example, padding [1, 2, 3, 4] with 2 elements on both sides in symmetric mode\n",
        "              will result in [2, 1, 1, 2, 3, 4, 4, 3]\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, padding, fill=0, padding_mode=\"constant\"):\n",
        "        super().__init__()\n",
        "        if not isinstance(padding, (numbers.Number, tuple, list)):\n",
        "            raise TypeError(\"Got inappropriate padding arg\")\n",
        "\n",
        "        if not isinstance(fill, (numbers.Number, str, tuple)):\n",
        "            raise TypeError(\"Got inappropriate fill arg\")\n",
        "\n",
        "        if padding_mode not in [\"constant\", \"edge\", \"reflect\", \"symmetric\"]:\n",
        "            raise ValueError(\"Padding mode should be either constant, edge, reflect or symmetric\")\n",
        "\n",
        "        if isinstance(padding, Sequence) and len(padding) not in [1, 2, 4]:\n",
        "            raise ValueError(\"Padding must be an int or a 1, 2, or 4 element tuple, not a \" +\n",
        "                             \"{} element tuple\".format(len(padding)))\n",
        "\n",
        "        self.padding = padding\n",
        "        self.fill = fill\n",
        "        self.padding_mode = padding_mode\n",
        "\n",
        "    def forward(self, img):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img (PIL Image or Tensor): Image to be padded.\n",
        "\n",
        "        Returns:\n",
        "            PIL Image or Tensor: Padded image.\n",
        "        \"\"\"\n",
        "        return F.pad(img, self.padding, self.fill, self.padding_mode)\n",
        "\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '(padding={0}, fill={1}, padding_mode={2})'.\\\n",
        "            format(self.padding, self.fill, self.padding_mode)\n",
        "\n",
        "\n",
        "\n",
        "class Lambda:\n",
        "    \"\"\"Apply a user-defined lambda as a transform. This transform does not support torchscript.\n",
        "\n",
        "    Args:\n",
        "        lambd (function): Lambda/function to be used for transform.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, lambd):\n",
        "        if not callable(lambd):\n",
        "            raise TypeError(\"Argument lambd should be callable, got {}\".format(repr(type(lambd).__name__)))\n",
        "        self.lambd = lambd\n",
        "\n",
        "    def __call__(self, img):\n",
        "        return self.lambd(img)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '()'\n",
        "\n",
        "\n",
        "\n",
        "class RandomTransforms:\n",
        "    \"\"\"Base class for a list of transformations with randomness\n",
        "\n",
        "    Args:\n",
        "        transforms (sequence): list of transformations\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, transforms):\n",
        "        if not isinstance(transforms, Sequence):\n",
        "            raise TypeError(\"Argument transforms should be a sequence\")\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __call__(self, *args, **kwargs):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def __repr__(self):\n",
        "        format_string = self.__class__.__name__ + '('\n",
        "        for t in self.transforms:\n",
        "            format_string += '\\n'\n",
        "            format_string += '    {0}'.format(t)\n",
        "        format_string += '\\n)'\n",
        "        return format_string\n",
        "\n",
        "\n",
        "class RandomApply(torch.nn.Module):\n",
        "    \"\"\"Apply randomly a list of transformations with a given probability.\n",
        "\n",
        "    .. note::\n",
        "        In order to script the transformation, please use ``torch.nn.ModuleList`` as input instead of list/tuple of\n",
        "        transforms as shown below:\n",
        "\n",
        "        >>> transforms = transforms.RandomApply(torch.nn.ModuleList([\n",
        "        >>>     transforms.ColorJitter(),\n",
        "        >>> ]), p=0.3)\n",
        "        >>> scripted_transforms = torch.jit.script(transforms)\n",
        "\n",
        "        Make sure to use only scriptable transformations, i.e. that work with ``torch.Tensor``, does not require\n",
        "        `lambda` functions or ``PIL.Image``.\n",
        "\n",
        "    Args:\n",
        "        transforms (sequence or torch.nn.Module): list of transformations\n",
        "        p (float): probability\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, transforms, p=0.5):\n",
        "        super().__init__()\n",
        "        self.transforms = transforms\n",
        "        self.p = p\n",
        "\n",
        "    def forward(self, img):\n",
        "        if self.p < torch.rand(1):\n",
        "            return img\n",
        "        for t in self.transforms:\n",
        "            img = t(img)\n",
        "        return img\n",
        "\n",
        "    def __repr__(self):\n",
        "        format_string = self.__class__.__name__ + '('\n",
        "        format_string += '\\n    p={}'.format(self.p)\n",
        "        for t in self.transforms:\n",
        "            format_string += '\\n'\n",
        "            format_string += '    {0}'.format(t)\n",
        "        format_string += '\\n)'\n",
        "        return format_string\n",
        "\n",
        "\n",
        "\n",
        "class RandomOrder(RandomTransforms):\n",
        "    \"\"\"Apply a list of transformations in a random order. This transform does not support torchscript.\n",
        "    \"\"\"\n",
        "    def __call__(self, img):\n",
        "        order = list(range(len(self.transforms)))\n",
        "        random.shuffle(order)\n",
        "        for i in order:\n",
        "            img = self.transforms[i](img)\n",
        "        return img\n",
        "\n",
        "\n",
        "\n",
        "class RandomChoice(RandomTransforms):\n",
        "    \"\"\"Apply single transformation randomly picked from a list. This transform does not support torchscript.\n",
        "    \"\"\"\n",
        "    def __init__(self, transforms, p=None):\n",
        "        super().__init__(transforms)\n",
        "        if p is not None and not isinstance(p, Sequence):\n",
        "            raise TypeError(\"Argument transforms should be a sequence\")\n",
        "        self.p = p\n",
        "\n",
        "    def __call__(self, *args):\n",
        "        t = random.choices(self.transforms, weights=self.p)[0]\n",
        "        return t(*args)\n",
        "\n",
        "    def __repr__(self):\n",
        "        format_string = super().__repr__()\n",
        "        format_string += '(p={0})'.format(self.p)\n",
        "        return format_string\n",
        "\n",
        "\n",
        "\n",
        "class RandomCrop(torch.nn.Module):\n",
        "    \"\"\"Crop the given image at a random location.\n",
        "    If the image is torch Tensor, it is expected\n",
        "    to have [..., H, W] shape, where ... means an arbitrary number of leading dimensions,\n",
        "    but if non-constant padding is used, the input is expected to have at most 2 leading dimensions\n",
        "\n",
        "    Args:\n",
        "        size (sequence or int): Desired output size of the crop. If size is an\n",
        "            int instead of sequence like (h, w), a square crop (size, size) is\n",
        "            made. If provided a sequence of length 1, it will be interpreted as (size[0], size[0]).\n",
        "        padding (int or sequence, optional): Optional padding on each border\n",
        "            of the image. Default is None. If a single int is provided this\n",
        "            is used to pad all borders. If sequence of length 2 is provided this is the padding\n",
        "            on left/right and top/bottom respectively. If a sequence of length 4 is provided\n",
        "            this is the padding for the left, top, right and bottom borders respectively.\n",
        "\n",
        "            .. note::\n",
        "                In torchscript mode padding as single int is not supported, use a sequence of\n",
        "                length 1: ``[padding, ]``.\n",
        "        pad_if_needed (boolean): It will pad the image if smaller than the\n",
        "            desired size to avoid raising an exception. Since cropping is done\n",
        "            after padding, the padding seems to be done at a random offset.\n",
        "        fill (number or str or tuple): Pixel fill value for constant fill. Default is 0. If a tuple of\n",
        "            length 3, it is used to fill R, G, B channels respectively.\n",
        "            This value is only used when the padding_mode is constant.\n",
        "            Only number is supported for torch Tensor.\n",
        "            Only int or str or tuple value is supported for PIL Image.\n",
        "        padding_mode (str): Type of padding. Should be: constant, edge, reflect or symmetric.\n",
        "            Default is constant.\n",
        "\n",
        "            - constant: pads with a constant value, this value is specified with fill\n",
        "\n",
        "            - edge: pads with the last value at the edge of the image.\n",
        "              If input a 5D torch Tensor, the last 3 dimensions will be padded instead of the last 2\n",
        "\n",
        "            - reflect: pads with reflection of image without repeating the last value on the edge.\n",
        "              For example, padding [1, 2, 3, 4] with 2 elements on both sides in reflect mode\n",
        "              will result in [3, 2, 1, 2, 3, 4, 3, 2]\n",
        "\n",
        "            - symmetric: pads with reflection of image repeating the last value on the edge.\n",
        "              For example, padding [1, 2, 3, 4] with 2 elements on both sides in symmetric mode\n",
        "              will result in [2, 1, 1, 2, 3, 4, 4, 3]\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def get_params(img: Tensor, output_size: Tuple[int, int]) -> Tuple[int, int, int, int]:\n",
        "        \"\"\"Get parameters for ``crop`` for a random crop.\n",
        "\n",
        "        Args:\n",
        "            img (PIL Image or Tensor): Image to be cropped.\n",
        "            output_size (tuple): Expected output size of the crop.\n",
        "\n",
        "        Returns:\n",
        "            tuple: params (i, j, h, w) to be passed to ``crop`` for random crop.\n",
        "        \"\"\"\n",
        "        w, h = F.get_image_size(img)\n",
        "        th, tw = output_size\n",
        "\n",
        "        if h + 1 < th or w + 1 < tw:\n",
        "            raise ValueError(\n",
        "                \"Required crop size {} is larger then input image size {}\".format((th, tw), (h, w))\n",
        "            )\n",
        "\n",
        "        if w == tw and h == th:\n",
        "            return 0, 0, h, w\n",
        "\n",
        "        i = torch.randint(0, h - th + 1, size=(1, )).item()\n",
        "        j = torch.randint(0, w - tw + 1, size=(1, )).item()\n",
        "        return i, j, th, tw\n",
        "\n",
        "\n",
        "    def __init__(self, size, padding=None, pad_if_needed=False, fill=0, padding_mode=\"constant\"):\n",
        "        super().__init__()\n",
        "\n",
        "        self.size = tuple(_setup_size(\n",
        "            size, error_msg=\"Please provide only two dimensions (h, w) for size.\"\n",
        "        ))\n",
        "\n",
        "        self.padding = padding\n",
        "        self.pad_if_needed = pad_if_needed\n",
        "        self.fill = fill\n",
        "        self.padding_mode = padding_mode\n",
        "\n",
        "    def forward(self, img):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img (PIL Image or Tensor): Image to be cropped.\n",
        "\n",
        "        Returns:\n",
        "            PIL Image or Tensor: Cropped image.\n",
        "        \"\"\"\n",
        "        if self.padding is not None:\n",
        "            img = F.pad(img, self.padding, self.fill, self.padding_mode)\n",
        "\n",
        "        width, height = F.get_image_size(img)\n",
        "        # pad the width if needed\n",
        "        if self.pad_if_needed and width < self.size[1]:\n",
        "            padding = [self.size[1] - width, 0]\n",
        "            img = F.pad(img, padding, self.fill, self.padding_mode)\n",
        "        # pad the height if needed\n",
        "        if self.pad_if_needed and height < self.size[0]:\n",
        "            padding = [0, self.size[0] - height]\n",
        "            img = F.pad(img, padding, self.fill, self.padding_mode)\n",
        "\n",
        "        i, j, h, w = self.get_params(img, self.size)\n",
        "\n",
        "        return F.crop(img, i, j, h, w)\n",
        "\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + \"(size={0}, padding={1})\".format(self.size, self.padding)\n",
        "\n",
        "\n",
        "\n",
        "class RandomHorizontalFlip(torch.nn.Module):\n",
        "    \"\"\"Horizontally flip the given image randomly with a given probability.\n",
        "    If the image is torch Tensor, it is expected\n",
        "    to have [..., H, W] shape, where ... means an arbitrary number of leading\n",
        "    dimensions\n",
        "\n",
        "    Args:\n",
        "        p (float): probability of the image being flipped. Default value is 0.5\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, p=0.5):\n",
        "        super().__init__()\n",
        "        self.p = p\n",
        "\n",
        "    def forward(self, img):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img (PIL Image or Tensor): Image to be flipped.\n",
        "\n",
        "        Returns:\n",
        "            PIL Image or Tensor: Randomly flipped image.\n",
        "        \"\"\"\n",
        "        if torch.rand(1) < self.p:\n",
        "            return F.hflip(img)\n",
        "        return img\n",
        "\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '(p={})'.format(self.p)\n",
        "\n",
        "\n",
        "\n",
        "class RandomVerticalFlip(torch.nn.Module):\n",
        "    \"\"\"Vertically flip the given image randomly with a given probability.\n",
        "    If the image is torch Tensor, it is expected\n",
        "    to have [..., H, W] shape, where ... means an arbitrary number of leading\n",
        "    dimensions\n",
        "\n",
        "    Args:\n",
        "        p (float): probability of the image being flipped. Default value is 0.5\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, p=0.5):\n",
        "        super().__init__()\n",
        "        self.p = p\n",
        "\n",
        "    def forward(self, img):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img (PIL Image or Tensor): Image to be flipped.\n",
        "\n",
        "        Returns:\n",
        "            PIL Image or Tensor: Randomly flipped image.\n",
        "        \"\"\"\n",
        "        if torch.rand(1) < self.p:\n",
        "            return F.vflip(img)\n",
        "        return img\n",
        "\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '(p={})'.format(self.p)\n",
        "\n",
        "\n",
        "\n",
        "class RandomPerspective(torch.nn.Module):\n",
        "    \"\"\"Performs a random perspective transformation of the given image with a given probability.\n",
        "    If the image is torch Tensor, it is expected\n",
        "    to have [..., H, W] shape, where ... means an arbitrary number of leading dimensions.\n",
        "\n",
        "    Args:\n",
        "        distortion_scale (float): argument to control the degree of distortion and ranges from 0 to 1.\n",
        "            Default is 0.5.\n",
        "        p (float): probability of the image being transformed. Default is 0.5.\n",
        "        interpolation (InterpolationMode): Desired interpolation enum defined by\n",
        "            :class:`torchvision.transforms.InterpolationMode`. Default is ``InterpolationMode.BILINEAR``.\n",
        "            If input is Tensor, only ``InterpolationMode.NEAREST``, ``InterpolationMode.BILINEAR`` are supported.\n",
        "            For backward compatibility integer values (e.g. ``PIL.Image.NEAREST``) are still acceptable.\n",
        "        fill (sequence or number): Pixel fill value for the area outside the transformed\n",
        "            image. Default is ``0``. If given a number, the value is used for all bands respectively.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, distortion_scale=0.5, p=0.5, interpolation=InterpolationMode.BILINEAR, fill=0):\n",
        "        super().__init__()\n",
        "        self.p = p\n",
        "\n",
        "        # Backward compatibility with integer value\n",
        "        if isinstance(interpolation, int):\n",
        "            warnings.warn(\n",
        "                \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
        "                \"Please, use InterpolationMode enum.\"\n",
        "            )\n",
        "            interpolation = _interpolation_modes_from_int(interpolation)\n",
        "\n",
        "        self.interpolation = interpolation\n",
        "        self.distortion_scale = distortion_scale\n",
        "\n",
        "        if fill is None:\n",
        "            fill = 0\n",
        "        elif not isinstance(fill, (Sequence, numbers.Number)):\n",
        "            raise TypeError(\"Fill should be either a sequence or a number.\")\n",
        "\n",
        "        self.fill = fill\n",
        "\n",
        "    def forward(self, img):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img (PIL Image or Tensor): Image to be Perspectively transformed.\n",
        "\n",
        "        Returns:\n",
        "            PIL Image or Tensor: Randomly transformed image.\n",
        "        \"\"\"\n",
        "\n",
        "        fill = self.fill\n",
        "        if isinstance(img, Tensor):\n",
        "            if isinstance(fill, (int, float)):\n",
        "                fill = [float(fill)] * F.get_image_num_channels(img)\n",
        "            else:\n",
        "                fill = [float(f) for f in fill]\n",
        "\n",
        "        if torch.rand(1) < self.p:\n",
        "            width, height = F.get_image_size(img)\n",
        "            startpoints, endpoints = self.get_params(width, height, self.distortion_scale)\n",
        "            return F.perspective(img, startpoints, endpoints, self.interpolation, fill)\n",
        "        return img\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def get_params(width: int, height: int, distortion_scale: float) -> Tuple[List[List[int]], List[List[int]]]:\n",
        "        \"\"\"Get parameters for ``perspective`` for a random perspective transform.\n",
        "\n",
        "        Args:\n",
        "            width (int): width of the image.\n",
        "            height (int): height of the image.\n",
        "            distortion_scale (float): argument to control the degree of distortion and ranges from 0 to 1.\n",
        "\n",
        "        Returns:\n",
        "            List containing [top-left, top-right, bottom-right, bottom-left] of the original image,\n",
        "            List containing [top-left, top-right, bottom-right, bottom-left] of the transformed image.\n",
        "        \"\"\"\n",
        "        half_height = height // 2\n",
        "        half_width = width // 2\n",
        "        topleft = [\n",
        "            int(torch.randint(0, int(distortion_scale * half_width) + 1, size=(1, )).item()),\n",
        "            int(torch.randint(0, int(distortion_scale * half_height) + 1, size=(1, )).item())\n",
        "        ]\n",
        "        topright = [\n",
        "            int(torch.randint(width - int(distortion_scale * half_width) - 1, width, size=(1, )).item()),\n",
        "            int(torch.randint(0, int(distortion_scale * half_height) + 1, size=(1, )).item())\n",
        "        ]\n",
        "        botright = [\n",
        "            int(torch.randint(width - int(distortion_scale * half_width) - 1, width, size=(1, )).item()),\n",
        "            int(torch.randint(height - int(distortion_scale * half_height) - 1, height, size=(1, )).item())\n",
        "        ]\n",
        "        botleft = [\n",
        "            int(torch.randint(0, int(distortion_scale * half_width) + 1, size=(1, )).item()),\n",
        "            int(torch.randint(height - int(distortion_scale * half_height) - 1, height, size=(1, )).item())\n",
        "        ]\n",
        "        startpoints = [[0, 0], [width - 1, 0], [width - 1, height - 1], [0, height - 1]]\n",
        "        endpoints = [topleft, topright, botright, botleft]\n",
        "        return startpoints, endpoints\n",
        "\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '(p={})'.format(self.p)\n",
        "\n",
        "\n",
        "\n",
        "class RandomResizedCrop(torch.nn.Module):\n",
        "    \"\"\"Crop a random portion of image and resize it to a given size.\n",
        "\n",
        "    If the image is torch Tensor, it is expected\n",
        "    to have [..., H, W] shape, where ... means an arbitrary number of leading dimensions\n",
        "\n",
        "    A crop of the original image is made: the crop has a random area (H * W)\n",
        "    and a random aspect ratio. This crop is finally resized to the given\n",
        "    size. This is popularly used to train the Inception networks.\n",
        "\n",
        "    Args:\n",
        "        size (int or sequence): expected output size of the crop, for each edge. If size is an\n",
        "            int instead of sequence like (h, w), a square output size ``(size, size)`` is\n",
        "            made. If provided a sequence of length 1, it will be interpreted as (size[0], size[0]).\n",
        "\n",
        "            .. note::\n",
        "                In torchscript mode size as single int is not supported, use a sequence of length 1: ``[size, ]``.\n",
        "        scale (tuple of float): Specifies the lower and upper bounds for the random area of the crop,\n",
        "            before resizing. The scale is defined with respect to the area of the original image.\n",
        "        ratio (tuple of float): lower and upper bounds for the random aspect ratio of the crop, before\n",
        "            resizing.\n",
        "        interpolation (InterpolationMode): Desired interpolation enum defined by\n",
        "            :class:`torchvision.transforms.InterpolationMode`. Default is ``InterpolationMode.BILINEAR``.\n",
        "            If input is Tensor, only ``InterpolationMode.NEAREST``, ``InterpolationMode.BILINEAR`` and\n",
        "            ``InterpolationMode.BICUBIC`` are supported.\n",
        "            For backward compatibility integer values (e.g. ``PIL.Image.NEAREST``) are still acceptable.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, size, scale=(0.08, 1.0), ratio=(3. / 4., 4. / 3.), interpolation=InterpolationMode.BILINEAR):\n",
        "        super().__init__()\n",
        "        self.size = _setup_size(size, error_msg=\"Please provide only two dimensions (h, w) for size.\")\n",
        "\n",
        "        if not isinstance(scale, Sequence):\n",
        "            raise TypeError(\"Scale should be a sequence\")\n",
        "        if not isinstance(ratio, Sequence):\n",
        "            raise TypeError(\"Ratio should be a sequence\")\n",
        "        if (scale[0] > scale[1]) or (ratio[0] > ratio[1]):\n",
        "            warnings.warn(\"Scale and ratio should be of kind (min, max)\")\n",
        "\n",
        "        # Backward compatibility with integer value\n",
        "        if isinstance(interpolation, int):\n",
        "            warnings.warn(\n",
        "                \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
        "                \"Please, use InterpolationMode enum.\"\n",
        "            )\n",
        "            interpolation = _interpolation_modes_from_int(interpolation)\n",
        "\n",
        "        self.interpolation = interpolation\n",
        "        self.scale = scale\n",
        "        self.ratio = ratio\n",
        "\n",
        "    @staticmethod\n",
        "    def get_params(\n",
        "            img: Tensor, scale: List[float], ratio: List[float]\n",
        "    ) -> Tuple[int, int, int, int]:\n",
        "        \"\"\"Get parameters for ``crop`` for a random sized crop.\n",
        "\n",
        "        Args:\n",
        "            img (PIL Image or Tensor): Input image.\n",
        "            scale (list): range of scale of the origin size cropped\n",
        "            ratio (list): range of aspect ratio of the origin aspect ratio cropped\n",
        "\n",
        "        Returns:\n",
        "            tuple: params (i, j, h, w) to be passed to ``crop`` for a random\n",
        "            sized crop.\n",
        "        \"\"\"\n",
        "        width, height = F.get_image_size(img)\n",
        "        area = height * width\n",
        "\n",
        "        log_ratio = torch.log(torch.tensor(ratio))\n",
        "        for _ in range(10):\n",
        "            target_area = area * torch.empty(1).uniform_(scale[0], scale[1]).item()\n",
        "            aspect_ratio = torch.exp(\n",
        "                torch.empty(1).uniform_(log_ratio[0], log_ratio[1])\n",
        "            ).item()\n",
        "\n",
        "            w = int(round(math.sqrt(target_area * aspect_ratio)))\n",
        "            h = int(round(math.sqrt(target_area / aspect_ratio)))\n",
        "\n",
        "            if 0 < w <= width and 0 < h <= height:\n",
        "                i = torch.randint(0, height - h + 1, size=(1,)).item()\n",
        "                j = torch.randint(0, width - w + 1, size=(1,)).item()\n",
        "                return i, j, h, w\n",
        "\n",
        "        # Fallback to central crop\n",
        "        in_ratio = float(width) / float(height)\n",
        "        if in_ratio < min(ratio):\n",
        "            w = width\n",
        "            h = int(round(w / min(ratio)))\n",
        "        elif in_ratio > max(ratio):\n",
        "            h = height\n",
        "            w = int(round(h * max(ratio)))\n",
        "        else:  # whole image\n",
        "            w = width\n",
        "            h = height\n",
        "        i = (height - h) // 2\n",
        "        j = (width - w) // 2\n",
        "        return i, j, h, w\n",
        "\n",
        "\n",
        "    def forward(self, img):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img (PIL Image or Tensor): Image to be cropped and resized.\n",
        "\n",
        "        Returns:\n",
        "            PIL Image or Tensor: Randomly cropped and resized image.\n",
        "        \"\"\"\n",
        "        i, j, h, w = self.get_params(img, self.scale, self.ratio)\n",
        "        return F.resized_crop(img, i, j, h, w, self.size, self.interpolation)\n",
        "\n",
        "\n",
        "    def __repr__(self):\n",
        "        interpolate_str = self.interpolation.value\n",
        "        format_string = self.__class__.__name__ + '(size={0}'.format(self.size)\n",
        "        format_string += ', scale={0}'.format(tuple(round(s, 4) for s in self.scale))\n",
        "        format_string += ', ratio={0}'.format(tuple(round(r, 4) for r in self.ratio))\n",
        "        format_string += ', interpolation={0})'.format(interpolate_str)\n",
        "        return format_string\n",
        "\n",
        "\n",
        "\n",
        "class RandomSizedCrop(RandomResizedCrop):\n",
        "    \"\"\"\n",
        "    Note: This transform is deprecated in favor of RandomResizedCrop.\n",
        "    \"\"\"\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        warnings.warn(\"The use of the transforms.RandomSizedCrop transform is deprecated, \" +\n",
        "                      \"please use transforms.RandomResizedCrop instead.\")\n",
        "        super(RandomSizedCrop, self).__init__(*args, **kwargs)\n",
        "\n",
        "\n",
        "\n",
        "class FiveCrop(torch.nn.Module):\n",
        "    \"\"\"Crop the given image into four corners and the central crop.\n",
        "    If the image is torch Tensor, it is expected\n",
        "    to have [..., H, W] shape, where ... means an arbitrary number of leading\n",
        "    dimensions\n",
        "\n",
        "    .. Note::\n",
        "         This transform returns a tuple of images and there may be a mismatch in the number of\n",
        "         inputs and targets your Dataset returns. See below for an example of how to deal with\n",
        "         this.\n",
        "\n",
        "    Args:\n",
        "         size (sequence or int): Desired output size of the crop. If size is an ``int``\n",
        "            instead of sequence like (h, w), a square crop of size (size, size) is made.\n",
        "            If provided a sequence of length 1, it will be interpreted as (size[0], size[0]).\n",
        "\n",
        "    Example:\n",
        "         >>> transform = Compose([\n",
        "         >>>    FiveCrop(size), # this is a list of PIL Images\n",
        "         >>>    Lambda(lambda crops: torch.stack([ToTensor()(crop) for crop in crops])) # returns a 4D tensor\n",
        "         >>> ])\n",
        "         >>> #In your test loop you can do the following:\n",
        "         >>> input, target = batch # input is a 5d tensor, target is 2d\n",
        "         >>> bs, ncrops, c, h, w = input.size()\n",
        "         >>> result = model(input.view(-1, c, h, w)) # fuse batch size and ncrops\n",
        "         >>> result_avg = result.view(bs, ncrops, -1).mean(1) # avg over crops\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, size):\n",
        "        super().__init__()\n",
        "        self.size = _setup_size(size, error_msg=\"Please provide only two dimensions (h, w) for size.\")\n",
        "\n",
        "    def forward(self, img):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img (PIL Image or Tensor): Image to be cropped.\n",
        "\n",
        "        Returns:\n",
        "            tuple of 5 images. Image can be PIL Image or Tensor\n",
        "        \"\"\"\n",
        "        return F.five_crop(img, self.size)\n",
        "\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '(size={0})'.format(self.size)\n",
        "\n",
        "\n",
        "\n",
        "class TenCrop(torch.nn.Module):\n",
        "    \"\"\"Crop the given image into four corners and the central crop plus the flipped version of\n",
        "    these (horizontal flipping is used by default).\n",
        "    If the image is torch Tensor, it is expected\n",
        "    to have [..., H, W] shape, where ... means an arbitrary number of leading\n",
        "    dimensions\n",
        "\n",
        "    .. Note::\n",
        "         This transform returns a tuple of images and there may be a mismatch in the number of\n",
        "         inputs and targets your Dataset returns. See below for an example of how to deal with\n",
        "         this.\n",
        "\n",
        "    Args:\n",
        "        size (sequence or int): Desired output size of the crop. If size is an\n",
        "            int instead of sequence like (h, w), a square crop (size, size) is\n",
        "            made. If provided a sequence of length 1, it will be interpreted as (size[0], size[0]).\n",
        "        vertical_flip (bool): Use vertical flipping instead of horizontal\n",
        "\n",
        "    Example:\n",
        "         >>> transform = Compose([\n",
        "         >>>    TenCrop(size), # this is a list of PIL Images\n",
        "         >>>    Lambda(lambda crops: torch.stack([ToTensor()(crop) for crop in crops])) # returns a 4D tensor\n",
        "         >>> ])\n",
        "         >>> #In your test loop you can do the following:\n",
        "         >>> input, target = batch # input is a 5d tensor, target is 2d\n",
        "         >>> bs, ncrops, c, h, w = input.size()\n",
        "         >>> result = model(input.view(-1, c, h, w)) # fuse batch size and ncrops\n",
        "         >>> result_avg = result.view(bs, ncrops, -1).mean(1) # avg over crops\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, size, vertical_flip=False):\n",
        "        super().__init__()\n",
        "        self.size = _setup_size(size, error_msg=\"Please provide only two dimensions (h, w) for size.\")\n",
        "        self.vertical_flip = vertical_flip\n",
        "\n",
        "    def forward(self, img):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img (PIL Image or Tensor): Image to be cropped.\n",
        "\n",
        "        Returns:\n",
        "            tuple of 10 images. Image can be PIL Image or Tensor\n",
        "        \"\"\"\n",
        "        return F.ten_crop(img, self.size, self.vertical_flip)\n",
        "\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '(size={0}, vertical_flip={1})'.format(self.size, self.vertical_flip)\n",
        "\n",
        "\n",
        "\n",
        "class LinearTransformation(torch.nn.Module):\n",
        "    \"\"\"Transform a tensor image with a square transformation matrix and a mean_vector computed\n",
        "    offline.\n",
        "    This transform does not support PIL Image.\n",
        "    Given transformation_matrix and mean_vector, will flatten the torch.*Tensor and\n",
        "    subtract mean_vector from it which is then followed by computing the dot\n",
        "    product with the transformation matrix and then reshaping the tensor to its\n",
        "    original shape.\n",
        "\n",
        "    Applications:\n",
        "        whitening transformation: Suppose X is a column vector zero-centered data.\n",
        "        Then compute the data covariance matrix [D x D] with torch.mm(X.t(), X),\n",
        "        perform SVD on this matrix and pass it as transformation_matrix.\n",
        "\n",
        "    Args:\n",
        "        transformation_matrix (Tensor): tensor [D x D], D = C x H x W\n",
        "        mean_vector (Tensor): tensor [D], D = C x H x W\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, transformation_matrix, mean_vector):\n",
        "        super().__init__()\n",
        "        if transformation_matrix.size(0) != transformation_matrix.size(1):\n",
        "            raise ValueError(\"transformation_matrix should be square. Got \" +\n",
        "                             \"[{} x {}] rectangular matrix.\".format(*transformation_matrix.size()))\n",
        "\n",
        "        if mean_vector.size(0) != transformation_matrix.size(0):\n",
        "            raise ValueError(\"mean_vector should have the same length {}\".format(mean_vector.size(0)) +\n",
        "                             \" as any one of the dimensions of the transformation_matrix [{}]\"\n",
        "                             .format(tuple(transformation_matrix.size())))\n",
        "\n",
        "        if transformation_matrix.device != mean_vector.device:\n",
        "            raise ValueError(\"Input tensors should be on the same device. Got {} and {}\"\n",
        "                             .format(transformation_matrix.device, mean_vector.device))\n",
        "\n",
        "        self.transformation_matrix = transformation_matrix\n",
        "        self.mean_vector = mean_vector\n",
        "\n",
        "    def forward(self, tensor: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            tensor (Tensor): Tensor image to be whitened.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Transformed image.\n",
        "        \"\"\"\n",
        "        shape = tensor.shape\n",
        "        n = shape[-3] * shape[-2] * shape[-1]\n",
        "        if n != self.transformation_matrix.shape[0]:\n",
        "            raise ValueError(\"Input tensor and transformation matrix have incompatible shape.\" +\n",
        "                             \"[{} x {} x {}] != \".format(shape[-3], shape[-2], shape[-1]) +\n",
        "                             \"{}\".format(self.transformation_matrix.shape[0]))\n",
        "\n",
        "        if tensor.device.type != self.mean_vector.device.type:\n",
        "            raise ValueError(\"Input tensor should be on the same device as transformation matrix and mean vector. \"\n",
        "                             \"Got {} vs {}\".format(tensor.device, self.mean_vector.device))\n",
        "\n",
        "        flat_tensor = tensor.view(-1, n) - self.mean_vector\n",
        "        transformed_tensor = torch.mm(flat_tensor, self.transformation_matrix)\n",
        "        tensor = transformed_tensor.view(shape)\n",
        "        return tensor\n",
        "\n",
        "\n",
        "    def __repr__(self):\n",
        "        format_string = self.__class__.__name__ + '(transformation_matrix='\n",
        "        format_string += (str(self.transformation_matrix.tolist()) + ')')\n",
        "        format_string += (\", (mean_vector=\" + str(self.mean_vector.tolist()) + ')')\n",
        "        return format_string\n",
        "\n",
        "\n",
        "\n",
        "class ColorJitter(torch.nn.Module):\n",
        "    \"\"\"Randomly change the brightness, contrast, saturation and hue of an image.\n",
        "    If the image is torch Tensor, it is expected\n",
        "    to have [..., 1 or 3, H, W] shape, where ... means an arbitrary number of leading dimensions.\n",
        "    If img is PIL Image, mode \"1\", \"I\", \"F\" and modes with transparency (alpha channel) are not supported.\n",
        "\n",
        "    Args:\n",
        "        brightness (float or tuple of float (min, max)): How much to jitter brightness.\n",
        "            brightness_factor is chosen uniformly from [max(0, 1 - brightness), 1 + brightness]\n",
        "            or the given [min, max]. Should be non negative numbers.\n",
        "        contrast (float or tuple of float (min, max)): How much to jitter contrast.\n",
        "            contrast_factor is chosen uniformly from [max(0, 1 - contrast), 1 + contrast]\n",
        "            or the given [min, max]. Should be non negative numbers.\n",
        "        saturation (float or tuple of float (min, max)): How much to jitter saturation.\n",
        "            saturation_factor is chosen uniformly from [max(0, 1 - saturation), 1 + saturation]\n",
        "            or the given [min, max]. Should be non negative numbers.\n",
        "        hue (float or tuple of float (min, max)): How much to jitter hue.\n",
        "            hue_factor is chosen uniformly from [-hue, hue] or the given [min, max].\n",
        "            Should have 0<= hue <= 0.5 or -0.5 <= min <= max <= 0.5.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, brightness=0, contrast=0, saturation=0, hue=0):\n",
        "        super().__init__()\n",
        "        self.brightness = self._check_input(brightness, 'brightness')\n",
        "        self.contrast = self._check_input(contrast, 'contrast')\n",
        "        self.saturation = self._check_input(saturation, 'saturation')\n",
        "        self.hue = self._check_input(hue, 'hue', center=0, bound=(-0.5, 0.5),\n",
        "                                     clip_first_on_zero=False)\n",
        "\n",
        "    @torch.jit.unused\n",
        "    def _check_input(self, value, name, center=1, bound=(0, float('inf')), clip_first_on_zero=True):\n",
        "        if isinstance(value, numbers.Number):\n",
        "            if value < 0:\n",
        "                raise ValueError(\"If {} is a single number, it must be non negative.\".format(name))\n",
        "            value = [center - float(value), center + float(value)]\n",
        "            if clip_first_on_zero:\n",
        "                value[0] = max(value[0], 0.0)\n",
        "        elif isinstance(value, (tuple, list)) and len(value) == 2:\n",
        "            if not bound[0] <= value[0] <= value[1] <= bound[1]:\n",
        "                raise ValueError(\"{} values should be between {}\".format(name, bound))\n",
        "        else:\n",
        "            raise TypeError(\"{} should be a single number or a list/tuple with length 2.\".format(name))\n",
        "\n",
        "        # if value is 0 or (1., 1.) for brightness/contrast/saturation\n",
        "        # or (0., 0.) for hue, do nothing\n",
        "        if value[0] == value[1] == center:\n",
        "            value = None\n",
        "        return value\n",
        "\n",
        "    @staticmethod\n",
        "    def get_params(brightness: Optional[List[float]],\n",
        "                   contrast: Optional[List[float]],\n",
        "                   saturation: Optional[List[float]],\n",
        "                   hue: Optional[List[float]]\n",
        "                   ) -> Tuple[Tensor, Optional[float], Optional[float], Optional[float], Optional[float]]:\n",
        "        \"\"\"Get the parameters for the randomized transform to be applied on image.\n",
        "\n",
        "        Args:\n",
        "            brightness (tuple of float (min, max), optional): The range from which the brightness_factor is chosen\n",
        "                uniformly. Pass None to turn off the transformation.\n",
        "            contrast (tuple of float (min, max), optional): The range from which the contrast_factor is chosen\n",
        "                uniformly. Pass None to turn off the transformation.\n",
        "            saturation (tuple of float (min, max), optional): The range from which the saturation_factor is chosen\n",
        "                uniformly. Pass None to turn off the transformation.\n",
        "            hue (tuple of float (min, max), optional): The range from which the hue_factor is chosen uniformly.\n",
        "                Pass None to turn off the transformation.\n",
        "\n",
        "        Returns:\n",
        "            tuple: The parameters used to apply the randomized transform\n",
        "            along with their random order.\n",
        "        \"\"\"\n",
        "        fn_idx = torch.randperm(4)\n",
        "\n",
        "        b = None if brightness is None else float(torch.empty(1).uniform_(brightness[0], brightness[1]))\n",
        "        c = None if contrast is None else float(torch.empty(1).uniform_(contrast[0], contrast[1]))\n",
        "        s = None if saturation is None else float(torch.empty(1).uniform_(saturation[0], saturation[1]))\n",
        "        h = None if hue is None else float(torch.empty(1).uniform_(hue[0], hue[1]))\n",
        "\n",
        "        return fn_idx, b, c, s, h\n",
        "\n",
        "\n",
        "    def forward(self, img):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img (PIL Image or Tensor): Input image.\n",
        "\n",
        "        Returns:\n",
        "            PIL Image or Tensor: Color jittered image.\n",
        "        \"\"\"\n",
        "        fn_idx, brightness_factor, contrast_factor, saturation_factor, hue_factor = \\\n",
        "            self.get_params(self.brightness, self.contrast, self.saturation, self.hue)\n",
        "\n",
        "        for fn_id in fn_idx:\n",
        "            if fn_id == 0 and brightness_factor is not None:\n",
        "                img = F.adjust_brightness(img, brightness_factor)\n",
        "            elif fn_id == 1 and contrast_factor is not None:\n",
        "                img = F.adjust_contrast(img, contrast_factor)\n",
        "            elif fn_id == 2 and saturation_factor is not None:\n",
        "                img = F.adjust_saturation(img, saturation_factor)\n",
        "            elif fn_id == 3 and hue_factor is not None:\n",
        "                img = F.adjust_hue(img, hue_factor)\n",
        "\n",
        "        return img\n",
        "\n",
        "\n",
        "    def __repr__(self):\n",
        "        format_string = self.__class__.__name__ + '('\n",
        "        format_string += 'brightness={0}'.format(self.brightness)\n",
        "        format_string += ', contrast={0}'.format(self.contrast)\n",
        "        format_string += ', saturation={0}'.format(self.saturation)\n",
        "        format_string += ', hue={0})'.format(self.hue)\n",
        "        return format_string\n",
        "\n",
        "\n",
        "\n",
        "class RandomRotation(torch.nn.Module):\n",
        "    \"\"\"Rotate the image by angle.\n",
        "    If the image is torch Tensor, it is expected\n",
        "    to have [..., H, W] shape, where ... means an arbitrary number of leading dimensions.\n",
        "\n",
        "    Args:\n",
        "        degrees (sequence or number): Range of degrees to select from.\n",
        "            If degrees is a number instead of sequence like (min, max), the range of degrees\n",
        "            will be (-degrees, +degrees).\n",
        "        interpolation (InterpolationMode): Desired interpolation enum defined by\n",
        "            :class:`torchvision.transforms.InterpolationMode`. Default is ``InterpolationMode.NEAREST``.\n",
        "            If input is Tensor, only ``InterpolationMode.NEAREST``, ``InterpolationMode.BILINEAR`` are supported.\n",
        "            For backward compatibility integer values (e.g. ``PIL.Image.NEAREST``) are still acceptable.\n",
        "        expand (bool, optional): Optional expansion flag.\n",
        "            If true, expands the output to make it large enough to hold the entire rotated image.\n",
        "            If false or omitted, make the output image the same size as the input image.\n",
        "            Note that the expand flag assumes rotation around the center and no translation.\n",
        "        center (sequence, optional): Optional center of rotation, (x, y). Origin is the upper left corner.\n",
        "            Default is the center of the image.\n",
        "        fill (sequence or number): Pixel fill value for the area outside the rotated\n",
        "            image. Default is ``0``. If given a number, the value is used for all bands respectively.\n",
        "        resample (int, optional): deprecated argument and will be removed since v0.10.0.\n",
        "            Please use the ``interpolation`` parameter instead.\n",
        "\n",
        "    .. _filters: https://pillow.readthedocs.io/en/latest/handbook/concepts.html#filters\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, degrees, interpolation=InterpolationMode.NEAREST, expand=False, center=None, fill=0, resample=None\n",
        "    ):\n",
        "        super().__init__()\n",
        "        if resample is not None:\n",
        "            warnings.warn(\n",
        "                \"Argument resample is deprecated and will be removed since v0.10.0. Please, use interpolation instead\"\n",
        "            )\n",
        "            interpolation = _interpolation_modes_from_int(resample)\n",
        "\n",
        "        # Backward compatibility with integer value\n",
        "        if isinstance(interpolation, int):\n",
        "            warnings.warn(\n",
        "                \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
        "                \"Please, use InterpolationMode enum.\"\n",
        "            )\n",
        "            interpolation = _interpolation_modes_from_int(interpolation)\n",
        "\n",
        "        self.degrees = _setup_angle(degrees, name=\"degrees\", req_sizes=(2, ))\n",
        "\n",
        "        if center is not None:\n",
        "            _check_sequence_input(center, \"center\", req_sizes=(2, ))\n",
        "\n",
        "        self.center = center\n",
        "\n",
        "        self.resample = self.interpolation = interpolation\n",
        "        self.expand = expand\n",
        "\n",
        "        if fill is None:\n",
        "            fill = 0\n",
        "        elif not isinstance(fill, (Sequence, numbers.Number)):\n",
        "            raise TypeError(\"Fill should be either a sequence or a number.\")\n",
        "\n",
        "        self.fill = fill\n",
        "\n",
        "    @staticmethod\n",
        "    def get_params(degrees: List[float]) -> float:\n",
        "        \"\"\"Get parameters for ``rotate`` for a random rotation.\n",
        "\n",
        "        Returns:\n",
        "            float: angle parameter to be passed to ``rotate`` for random rotation.\n",
        "        \"\"\"\n",
        "        angle = float(torch.empty(1).uniform_(float(degrees[0]), float(degrees[1])).item())\n",
        "        return angle\n",
        "\n",
        "\n",
        "    def forward(self, img):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img (PIL Image or Tensor): Image to be rotated.\n",
        "\n",
        "        Returns:\n",
        "            PIL Image or Tensor: Rotated image.\n",
        "        \"\"\"\n",
        "        fill = self.fill\n",
        "        if isinstance(img, Tensor):\n",
        "            if isinstance(fill, (int, float)):\n",
        "                fill = [float(fill)] * F.get_image_num_channels(img)\n",
        "            else:\n",
        "                fill = [float(f) for f in fill]\n",
        "        angle = self.get_params(self.degrees)\n",
        "\n",
        "        return F.rotate(img, angle, self.resample, self.expand, self.center, fill)\n",
        "\n",
        "\n",
        "    def __repr__(self):\n",
        "        interpolate_str = self.interpolation.value\n",
        "        format_string = self.__class__.__name__ + '(degrees={0}'.format(self.degrees)\n",
        "        format_string += ', interpolation={0}'.format(interpolate_str)\n",
        "        format_string += ', expand={0}'.format(self.expand)\n",
        "        if self.center is not None:\n",
        "            format_string += ', center={0}'.format(self.center)\n",
        "        if self.fill is not None:\n",
        "            format_string += ', fill={0}'.format(self.fill)\n",
        "        format_string += ')'\n",
        "        return format_string\n",
        "\n",
        "\n",
        "\n",
        "class RandomAffine(torch.nn.Module):\n",
        "    \"\"\"Random affine transformation of the image keeping center invariant.\n",
        "    If the image is torch Tensor, it is expected\n",
        "    to have [..., H, W] shape, where ... means an arbitrary number of leading dimensions.\n",
        "\n",
        "    Args:\n",
        "        degrees (sequence or number): Range of degrees to select from.\n",
        "            If degrees is a number instead of sequence like (min, max), the range of degrees\n",
        "            will be (-degrees, +degrees). Set to 0 to deactivate rotations.\n",
        "        translate (tuple, optional): tuple of maximum absolute fraction for horizontal\n",
        "            and vertical translations. For example translate=(a, b), then horizontal shift\n",
        "            is randomly sampled in the range -img_width * a < dx < img_width * a and vertical shift is\n",
        "            randomly sampled in the range -img_height * b < dy < img_height * b. Will not translate by default.\n",
        "        scale (tuple, optional): scaling factor interval, e.g (a, b), then scale is\n",
        "            randomly sampled from the range a <= scale <= b. Will keep original scale by default.\n",
        "        shear (sequence or number, optional): Range of degrees to select from.\n",
        "            If shear is a number, a shear parallel to the x axis in the range (-shear, +shear)\n",
        "            will be applied. Else if shear is a sequence of 2 values a shear parallel to the x axis in the\n",
        "            range (shear[0], shear[1]) will be applied. Else if shear is a sequence of 4 values,\n",
        "            a x-axis shear in (shear[0], shear[1]) and y-axis shear in (shear[2], shear[3]) will be applied.\n",
        "            Will not apply shear by default.\n",
        "        interpolation (InterpolationMode): Desired interpolation enum defined by\n",
        "            :class:`torchvision.transforms.InterpolationMode`. Default is ``InterpolationMode.NEAREST``.\n",
        "            If input is Tensor, only ``InterpolationMode.NEAREST``, ``InterpolationMode.BILINEAR`` are supported.\n",
        "            For backward compatibility integer values (e.g. ``PIL.Image.NEAREST``) are still acceptable.\n",
        "        fill (sequence or number): Pixel fill value for the area outside the transformed\n",
        "            image. Default is ``0``. If given a number, the value is used for all bands respectively.\n",
        "        fillcolor (sequence or number, optional): deprecated argument and will be removed since v0.10.0.\n",
        "            Please use the ``fill`` parameter instead.\n",
        "        resample (int, optional): deprecated argument and will be removed since v0.10.0.\n",
        "            Please use the ``interpolation`` parameter instead.\n",
        "\n",
        "    .. _filters: https://pillow.readthedocs.io/en/latest/handbook/concepts.html#filters\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, degrees, translate=None, scale=None, shear=None, interpolation=InterpolationMode.NEAREST, fill=0,\n",
        "        fillcolor=None, resample=None\n",
        "    ):\n",
        "        super().__init__()\n",
        "        if resample is not None:\n",
        "            warnings.warn(\n",
        "                \"Argument resample is deprecated and will be removed since v0.10.0. Please, use interpolation instead\"\n",
        "            )\n",
        "            interpolation = _interpolation_modes_from_int(resample)\n",
        "\n",
        "        # Backward compatibility with integer value\n",
        "        if isinstance(interpolation, int):\n",
        "            warnings.warn(\n",
        "                \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
        "                \"Please, use InterpolationMode enum.\"\n",
        "            )\n",
        "            interpolation = _interpolation_modes_from_int(interpolation)\n",
        "\n",
        "        if fillcolor is not None:\n",
        "            warnings.warn(\n",
        "                \"Argument fillcolor is deprecated and will be removed since v0.10.0. Please, use fill instead\"\n",
        "            )\n",
        "            fill = fillcolor\n",
        "\n",
        "        self.degrees = _setup_angle(degrees, name=\"degrees\", req_sizes=(2, ))\n",
        "\n",
        "        if translate is not None:\n",
        "            _check_sequence_input(translate, \"translate\", req_sizes=(2, ))\n",
        "            for t in translate:\n",
        "                if not (0.0 <= t <= 1.0):\n",
        "                    raise ValueError(\"translation values should be between 0 and 1\")\n",
        "        self.translate = translate\n",
        "\n",
        "        if scale is not None:\n",
        "            _check_sequence_input(scale, \"scale\", req_sizes=(2, ))\n",
        "            for s in scale:\n",
        "                if s <= 0:\n",
        "                    raise ValueError(\"scale values should be positive\")\n",
        "        self.scale = scale\n",
        "\n",
        "        if shear is not None:\n",
        "            self.shear = _setup_angle(shear, name=\"shear\", req_sizes=(2, 4))\n",
        "        else:\n",
        "            self.shear = shear\n",
        "\n",
        "        self.resample = self.interpolation = interpolation\n",
        "\n",
        "        if fill is None:\n",
        "            fill = 0\n",
        "        elif not isinstance(fill, (Sequence, numbers.Number)):\n",
        "            raise TypeError(\"Fill should be either a sequence or a number.\")\n",
        "\n",
        "        self.fillcolor = self.fill = fill\n",
        "\n",
        "    @staticmethod\n",
        "    def get_params(\n",
        "            degrees: List[float],\n",
        "            translate: Optional[List[float]],\n",
        "            scale_ranges: Optional[List[float]],\n",
        "            shears: Optional[List[float]],\n",
        "            img_size: List[int]\n",
        "    ) -> Tuple[float, Tuple[int, int], float, Tuple[float, float]]:\n",
        "        \"\"\"Get parameters for affine transformation\n",
        "\n",
        "        Returns:\n",
        "            params to be passed to the affine transformation\n",
        "        \"\"\"\n",
        "        angle = float(torch.empty(1).uniform_(float(degrees[0]), float(degrees[1])).item())\n",
        "        if translate is not None:\n",
        "            max_dx = float(translate[0] * img_size[0])\n",
        "            max_dy = float(translate[1] * img_size[1])\n",
        "            tx = int(round(torch.empty(1).uniform_(-max_dx, max_dx).item()))\n",
        "            ty = int(round(torch.empty(1).uniform_(-max_dy, max_dy).item()))\n",
        "            translations = (tx, ty)\n",
        "        else:\n",
        "            translations = (0, 0)\n",
        "\n",
        "        if scale_ranges is not None:\n",
        "            scale = float(torch.empty(1).uniform_(scale_ranges[0], scale_ranges[1]).item())\n",
        "        else:\n",
        "            scale = 1.0\n",
        "\n",
        "        shear_x = shear_y = 0.0\n",
        "        if shears is not None:\n",
        "            shear_x = float(torch.empty(1).uniform_(shears[0], shears[1]).item())\n",
        "            if len(shears) == 4:\n",
        "                shear_y = float(torch.empty(1).uniform_(shears[2], shears[3]).item())\n",
        "\n",
        "        shear = (shear_x, shear_y)\n",
        "\n",
        "        return angle, translations, scale, shear\n",
        "\n",
        "\n",
        "    def forward(self, img):\n",
        "        \"\"\"\n",
        "            img (PIL Image or Tensor): Image to be transformed.\n",
        "\n",
        "        Returns:\n",
        "            PIL Image or Tensor: Affine transformed image.\n",
        "        \"\"\"\n",
        "        fill = self.fill\n",
        "        if isinstance(img, Tensor):\n",
        "            if isinstance(fill, (int, float)):\n",
        "                fill = [float(fill)] * F.get_image_num_channels(img)\n",
        "            else:\n",
        "                fill = [float(f) for f in fill]\n",
        "\n",
        "        img_size = F.get_image_size(img)\n",
        "\n",
        "        ret = self.get_params(self.degrees, self.translate, self.scale, self.shear, img_size)\n",
        "\n",
        "        return F.affine(img, *ret, interpolation=self.interpolation, fill=fill)\n",
        "\n",
        "\n",
        "    def __repr__(self):\n",
        "        s = '{name}(degrees={degrees}'\n",
        "        if self.translate is not None:\n",
        "            s += ', translate={translate}'\n",
        "        if self.scale is not None:\n",
        "            s += ', scale={scale}'\n",
        "        if self.shear is not None:\n",
        "            s += ', shear={shear}'\n",
        "        if self.interpolation != InterpolationMode.NEAREST:\n",
        "            s += ', interpolation={interpolation}'\n",
        "        if self.fill != 0:\n",
        "            s += ', fill={fill}'\n",
        "        s += ')'\n",
        "        d = dict(self.__dict__)\n",
        "        d['interpolation'] = self.interpolation.value\n",
        "        return s.format(name=self.__class__.__name__, **d)\n",
        "\n",
        "\n",
        "\n",
        "class Grayscale(torch.nn.Module):\n",
        "    \"\"\"Convert image to grayscale.\n",
        "    If the image is torch Tensor, it is expected\n",
        "    to have [..., 3, H, W] shape, where ... means an arbitrary number of leading dimensions\n",
        "\n",
        "    Args:\n",
        "        num_output_channels (int): (1 or 3) number of channels desired for output image\n",
        "\n",
        "    Returns:\n",
        "        PIL Image: Grayscale version of the input.\n",
        "\n",
        "        - If ``num_output_channels == 1`` : returned image is single channel\n",
        "        - If ``num_output_channels == 3`` : returned image is 3 channel with r == g == b\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_output_channels=1):\n",
        "        super().__init__()\n",
        "        self.num_output_channels = num_output_channels\n",
        "\n",
        "    def forward(self, img):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img (PIL Image or Tensor): Image to be converted to grayscale.\n",
        "\n",
        "        Returns:\n",
        "            PIL Image or Tensor: Grayscaled image.\n",
        "        \"\"\"\n",
        "        return F.rgb_to_grayscale(img, num_output_channels=self.num_output_channels)\n",
        "\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '(num_output_channels={0})'.format(self.num_output_channels)\n",
        "\n",
        "\n",
        "\n",
        "class RandomGrayscale(torch.nn.Module):\n",
        "    \"\"\"Randomly convert image to grayscale with a probability of p (default 0.1).\n",
        "    If the image is torch Tensor, it is expected\n",
        "    to have [..., 3, H, W] shape, where ... means an arbitrary number of leading dimensions\n",
        "\n",
        "    Args:\n",
        "        p (float): probability that image should be converted to grayscale.\n",
        "\n",
        "    Returns:\n",
        "        PIL Image or Tensor: Grayscale version of the input image with probability p and unchanged\n",
        "        with probability (1-p).\n",
        "        - If input image is 1 channel: grayscale version is 1 channel\n",
        "        - If input image is 3 channel: grayscale version is 3 channel with r == g == b\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, p=0.1):\n",
        "        super().__init__()\n",
        "        self.p = p\n",
        "\n",
        "    def forward(self, img):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img (PIL Image or Tensor): Image to be converted to grayscale.\n",
        "\n",
        "        Returns:\n",
        "            PIL Image or Tensor: Randomly grayscaled image.\n",
        "        \"\"\"\n",
        "        num_output_channels = F.get_image_num_channels(img)\n",
        "        if torch.rand(1) < self.p:\n",
        "            return F.rgb_to_grayscale(img, num_output_channels=num_output_channels)\n",
        "        return img\n",
        "\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '(p={0})'.format(self.p)\n",
        "\n",
        "\n",
        "\n",
        "class RandomErasing(torch.nn.Module):\n",
        "    \"\"\" Randomly selects a rectangle region in an torch Tensor image and erases its pixels.\n",
        "    This transform does not support PIL Image.\n",
        "    'Random Erasing Data Augmentation' by Zhong et al. See https://arxiv.org/abs/1708.04896\n",
        "\n",
        "    Args:\n",
        "         p: probability that the random erasing operation will be performed.\n",
        "         scale: range of proportion of erased area against input image.\n",
        "         ratio: range of aspect ratio of erased area.\n",
        "         value: erasing value. Default is 0. If a single int, it is used to\n",
        "            erase all pixels. If a tuple of length 3, it is used to erase\n",
        "            R, G, B channels respectively.\n",
        "            If a str of 'random', erasing each pixel with random values.\n",
        "         inplace: boolean to make this transform inplace. Default set to False.\n",
        "\n",
        "    Returns:\n",
        "        Erased Image.\n",
        "\n",
        "    Example:\n",
        "        >>> transform = transforms.Compose([\n",
        "        >>>   transforms.RandomHorizontalFlip(),\n",
        "        >>>   transforms.PILToTensor(),\n",
        "        >>>   transforms.ConvertImageDtype(torch.float),\n",
        "        >>>   transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
        "        >>>   transforms.RandomErasing(),\n",
        "        >>> ])\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False):\n",
        "        super().__init__()\n",
        "        if not isinstance(value, (numbers.Number, str, tuple, list)):\n",
        "            raise TypeError(\"Argument value should be either a number or str or a sequence\")\n",
        "        if isinstance(value, str) and value != \"random\":\n",
        "            raise ValueError(\"If value is str, it should be 'random'\")\n",
        "        if not isinstance(scale, (tuple, list)):\n",
        "            raise TypeError(\"Scale should be a sequence\")\n",
        "        if not isinstance(ratio, (tuple, list)):\n",
        "            raise TypeError(\"Ratio should be a sequence\")\n",
        "        if (scale[0] > scale[1]) or (ratio[0] > ratio[1]):\n",
        "            warnings.warn(\"Scale and ratio should be of kind (min, max)\")\n",
        "        if scale[0] < 0 or scale[1] > 1:\n",
        "            raise ValueError(\"Scale should be between 0 and 1\")\n",
        "        if p < 0 or p > 1:\n",
        "            raise ValueError(\"Random erasing probability should be between 0 and 1\")\n",
        "\n",
        "        self.p = p\n",
        "        self.scale = scale\n",
        "        self.ratio = ratio\n",
        "        self.value = value\n",
        "        self.inplace = inplace\n",
        "\n",
        "    @staticmethod\n",
        "    def get_params(\n",
        "            img: Tensor, scale: Tuple[float, float], ratio: Tuple[float, float], value: Optional[List[float]] = None\n",
        "    ) -> Tuple[int, int, int, int, Tensor]:\n",
        "        \"\"\"Get parameters for ``erase`` for a random erasing.\n",
        "\n",
        "        Args:\n",
        "            img (Tensor): Tensor image to be erased.\n",
        "            scale (sequence): range of proportion of erased area against input image.\n",
        "            ratio (sequence): range of aspect ratio of erased area.\n",
        "            value (list, optional): erasing value. If None, it is interpreted as \"random\"\n",
        "                (erasing each pixel with random values). If ``len(value)`` is 1, it is interpreted as a number,\n",
        "                i.e. ``value[0]``.\n",
        "\n",
        "        Returns:\n",
        "            tuple: params (i, j, h, w, v) to be passed to ``erase`` for random erasing.\n",
        "        \"\"\"\n",
        "        img_c, img_h, img_w = img.shape[-3], img.shape[-2], img.shape[-1]\n",
        "        area = img_h * img_w\n",
        "\n",
        "        log_ratio = torch.log(torch.tensor(ratio))\n",
        "        for _ in range(10):\n",
        "            erase_area = area * torch.empty(1).uniform_(scale[0], scale[1]).item()\n",
        "            aspect_ratio = torch.exp(\n",
        "                torch.empty(1).uniform_(log_ratio[0], log_ratio[1])\n",
        "            ).item()\n",
        "\n",
        "            h = int(round(math.sqrt(erase_area * aspect_ratio)))\n",
        "            w = int(round(math.sqrt(erase_area / aspect_ratio)))\n",
        "            if not (h < img_h and w < img_w):\n",
        "                continue\n",
        "\n",
        "            if value is None:\n",
        "                v = torch.empty([img_c, h, w], dtype=torch.float32).normal_()\n",
        "            else:\n",
        "                v = torch.tensor(value)[:, None, None]\n",
        "\n",
        "            i = torch.randint(0, img_h - h + 1, size=(1, )).item()\n",
        "            j = torch.randint(0, img_w - w + 1, size=(1, )).item()\n",
        "            return i, j, h, w, v\n",
        "\n",
        "        # Return original image\n",
        "        return 0, 0, img_h, img_w, img\n",
        "\n",
        "\n",
        "    def forward(self, img):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img (Tensor): Tensor image to be erased.\n",
        "\n",
        "        Returns:\n",
        "            img (Tensor): Erased Tensor image.\n",
        "        \"\"\"\n",
        "        if torch.rand(1) < self.p:\n",
        "\n",
        "            # cast self.value to script acceptable type\n",
        "            if isinstance(self.value, (int, float)):\n",
        "                value = [self.value, ]\n",
        "            elif isinstance(self.value, str):\n",
        "                value = None\n",
        "            elif isinstance(self.value, tuple):\n",
        "                value = list(self.value)\n",
        "            else:\n",
        "                value = self.value\n",
        "\n",
        "            if value is not None and not (len(value) in (1, img.shape[-3])):\n",
        "                raise ValueError(\n",
        "                    \"If value is a sequence, it should have either a single value or \"\n",
        "                    \"{} (number of input channels)\".format(img.shape[-3])\n",
        "                )\n",
        "\n",
        "            x, y, h, w, v = self.get_params(img, scale=self.scale, ratio=self.ratio, value=value)\n",
        "            return F.erase(img, x, y, h, w, v, self.inplace)\n",
        "        return img\n",
        "\n",
        "\n",
        "    def __repr__(self):\n",
        "        s = '(p={}, '.format(self.p)\n",
        "        s += 'scale={}, '.format(self.scale)\n",
        "        s += 'ratio={}, '.format(self.ratio)\n",
        "        s += 'value={}, '.format(self.value)\n",
        "        s += 'inplace={})'.format(self.inplace)\n",
        "        return self.__class__.__name__ + s\n",
        "\n",
        "\n",
        "\n",
        "class GaussianBlur(torch.nn.Module):\n",
        "    \"\"\"Blurs image with randomly chosen Gaussian blur.\n",
        "    If the image is torch Tensor, it is expected\n",
        "    to have [..., C, H, W] shape, where ... means an arbitrary number of leading dimensions.\n",
        "\n",
        "    Args:\n",
        "        kernel_size (int or sequence): Size of the Gaussian kernel.\n",
        "        sigma (float or tuple of float (min, max)): Standard deviation to be used for\n",
        "            creating kernel to perform blurring. If float, sigma is fixed. If it is tuple\n",
        "            of float (min, max), sigma is chosen uniformly at random to lie in the\n",
        "            given range.\n",
        "\n",
        "    Returns:\n",
        "        PIL Image or Tensor: Gaussian blurred version of the input image.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, kernel_size, sigma=(0.1, 2.0)):\n",
        "        super().__init__()\n",
        "        self.kernel_size = _setup_size(kernel_size, \"Kernel size should be a tuple/list of two integers\")\n",
        "        for ks in self.kernel_size:\n",
        "            if ks <= 0 or ks % 2 == 0:\n",
        "                raise ValueError(\"Kernel size value should be an odd and positive number.\")\n",
        "\n",
        "        if isinstance(sigma, numbers.Number):\n",
        "            if sigma <= 0:\n",
        "                raise ValueError(\"If sigma is a single number, it must be positive.\")\n",
        "            sigma = (sigma, sigma)\n",
        "        elif isinstance(sigma, Sequence) and len(sigma) == 2:\n",
        "            if not 0. < sigma[0] <= sigma[1]:\n",
        "                raise ValueError(\"sigma values should be positive and of the form (min, max).\")\n",
        "        else:\n",
        "            raise ValueError(\"sigma should be a single number or a list/tuple with length 2.\")\n",
        "\n",
        "        self.sigma = sigma\n",
        "\n",
        "    @staticmethod\n",
        "    def get_params(sigma_min: float, sigma_max: float) -> float:\n",
        "        \"\"\"Choose sigma for random gaussian blurring.\n",
        "\n",
        "        Args:\n",
        "            sigma_min (float): Minimum standard deviation that can be chosen for blurring kernel.\n",
        "            sigma_max (float): Maximum standard deviation that can be chosen for blurring kernel.\n",
        "\n",
        "        Returns:\n",
        "            float: Standard deviation to be passed to calculate kernel for gaussian blurring.\n",
        "        \"\"\"\n",
        "        return torch.empty(1).uniform_(sigma_min, sigma_max).item()\n",
        "\n",
        "\n",
        "    def forward(self, img: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img (PIL Image or Tensor): image to be blurred.\n",
        "\n",
        "        Returns:\n",
        "            PIL Image or Tensor: Gaussian blurred image\n",
        "        \"\"\"\n",
        "        sigma = self.get_params(self.sigma[0], self.sigma[1])\n",
        "        return F.gaussian_blur(img, self.kernel_size, [sigma, sigma])\n",
        "\n",
        "\n",
        "    def __repr__(self):\n",
        "        s = '(kernel_size={}, '.format(self.kernel_size)\n",
        "        s += 'sigma={})'.format(self.sigma)\n",
        "        return self.__class__.__name__ + s\n",
        "\n",
        "\n",
        "\n",
        "def _setup_size(size, error_msg):\n",
        "    if isinstance(size, numbers.Number):\n",
        "        return int(size), int(size)\n",
        "\n",
        "    if isinstance(size, Sequence) and len(size) == 1:\n",
        "        return size[0], size[0]\n",
        "\n",
        "    if len(size) != 2:\n",
        "        raise ValueError(error_msg)\n",
        "\n",
        "    return size\n",
        "\n",
        "\n",
        "def _check_sequence_input(x, name, req_sizes):\n",
        "    msg = req_sizes[0] if len(req_sizes) < 2 else \" or \".join([str(s) for s in req_sizes])\n",
        "    if not isinstance(x, Sequence):\n",
        "        raise TypeError(\"{} should be a sequence of length {}.\".format(name, msg))\n",
        "    if len(x) not in req_sizes:\n",
        "        raise ValueError(\"{} should be sequence of length {}.\".format(name, msg))\n",
        "\n",
        "\n",
        "def _setup_angle(x, name, req_sizes=(2, )):\n",
        "    if isinstance(x, numbers.Number):\n",
        "        if x < 0:\n",
        "            raise ValueError(\"If {} is a single number, it must be positive.\".format(name))\n",
        "        x = [-x, x]\n",
        "    else:\n",
        "        _check_sequence_input(x, name, req_sizes)\n",
        "\n",
        "    return [float(d) for d in x]\n",
        "\n",
        "\n",
        "class RandomInvert(torch.nn.Module):\n",
        "    \"\"\"Inverts the colors of the given image randomly with a given probability.\n",
        "    If img is a Tensor, it is expected to be in [..., 1 or 3, H, W] format,\n",
        "    where ... means it can have an arbitrary number of leading dimensions.\n",
        "    If img is PIL Image, it is expected to be in mode \"L\" or \"RGB\".\n",
        "\n",
        "    Args:\n",
        "        p (float): probability of the image being color inverted. Default value is 0.5\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, p=0.5):\n",
        "        super().__init__()\n",
        "        self.p = p\n",
        "\n",
        "    def forward(self, img):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img (PIL Image or Tensor): Image to be inverted.\n",
        "\n",
        "        Returns:\n",
        "            PIL Image or Tensor: Randomly color inverted image.\n",
        "        \"\"\"\n",
        "        if torch.rand(1).item() < self.p:\n",
        "            return F.invert(img)\n",
        "        return img\n",
        "\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '(p={})'.format(self.p)\n",
        "\n",
        "\n",
        "\n",
        "class RandomPosterize(torch.nn.Module):\n",
        "    \"\"\"Posterize the image randomly with a given probability by reducing the\n",
        "    number of bits for each color channel. If the image is torch Tensor, it should be of type torch.uint8,\n",
        "    and it is expected to have [..., 1 or 3, H, W] shape, where ... means an arbitrary number of leading dimensions.\n",
        "    If img is PIL Image, it is expected to be in mode \"L\" or \"RGB\".\n",
        "\n",
        "    Args:\n",
        "        bits (int): number of bits to keep for each channel (0-8)\n",
        "        p (float): probability of the image being color inverted. Default value is 0.5\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, bits, p=0.5):\n",
        "        super().__init__()\n",
        "        self.bits = bits\n",
        "        self.p = p\n",
        "\n",
        "    def forward(self, img):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img (PIL Image or Tensor): Image to be posterized.\n",
        "\n",
        "        Returns:\n",
        "            PIL Image or Tensor: Randomly posterized image.\n",
        "        \"\"\"\n",
        "        if torch.rand(1).item() < self.p:\n",
        "            return F.posterize(img, self.bits)\n",
        "        return img\n",
        "\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '(bits={},p={})'.format(self.bits, self.p)\n",
        "\n",
        "\n",
        "\n",
        "class RandomSolarize(torch.nn.Module):\n",
        "    \"\"\"Solarize the image randomly with a given probability by inverting all pixel\n",
        "    values above a threshold. If img is a Tensor, it is expected to be in [..., 1 or 3, H, W] format,\n",
        "    where ... means it can have an arbitrary number of leading dimensions.\n",
        "    If img is PIL Image, it is expected to be in mode \"L\" or \"RGB\".\n",
        "\n",
        "    Args:\n",
        "        threshold (float): all pixels equal or above this value are inverted.\n",
        "        p (float): probability of the image being color inverted. Default value is 0.5\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, threshold, p=0.5):\n",
        "        super().__init__()\n",
        "        self.threshold = threshold\n",
        "        self.p = p\n",
        "\n",
        "    def forward(self, img):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img (PIL Image or Tensor): Image to be solarized.\n",
        "\n",
        "        Returns:\n",
        "            PIL Image or Tensor: Randomly solarized image.\n",
        "        \"\"\"\n",
        "        if torch.rand(1).item() < self.p:\n",
        "            return F.solarize(img, self.threshold)\n",
        "        return img\n",
        "\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '(threshold={},p={})'.format(self.threshold, self.p)\n",
        "\n",
        "\n",
        "\n",
        "class RandomAdjustSharpness(torch.nn.Module):\n",
        "    \"\"\"Adjust the sharpness of the image randomly with a given probability. If the image is torch Tensor,\n",
        "    it is expected to have [..., 1 or 3, H, W] shape, where ... means an arbitrary number of leading dimensions.\n",
        "\n",
        "    Args:\n",
        "        sharpness_factor (float):  How much to adjust the sharpness. Can be\n",
        "            any non negative number. 0 gives a blurred image, 1 gives the\n",
        "            original image while 2 increases the sharpness by a factor of 2.\n",
        "        p (float): probability of the image being color inverted. Default value is 0.5\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, sharpness_factor, p=0.5):\n",
        "        super().__init__()\n",
        "        self.sharpness_factor = sharpness_factor\n",
        "        self.p = p\n",
        "\n",
        "    def forward(self, img):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img (PIL Image or Tensor): Image to be sharpened.\n",
        "\n",
        "        Returns:\n",
        "            PIL Image or Tensor: Randomly sharpened image.\n",
        "        \"\"\"\n",
        "        if torch.rand(1).item() < self.p:\n",
        "            return F.adjust_sharpness(img, self.sharpness_factor)\n",
        "        return img\n",
        "\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '(sharpness_factor={},p={})'.format(self.sharpness_factor, self.p)\n",
        "\n",
        "\n",
        "\n",
        "class RandomAutocontrast(torch.nn.Module):\n",
        "    \"\"\"Autocontrast the pixels of the given image randomly with a given probability.\n",
        "    If the image is torch Tensor, it is expected\n",
        "    to have [..., 1 or 3, H, W] shape, where ... means an arbitrary number of leading dimensions.\n",
        "    If img is PIL Image, it is expected to be in mode \"L\" or \"RGB\".\n",
        "\n",
        "    Args:\n",
        "        p (float): probability of the image being autocontrasted. Default value is 0.5\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, p=0.5):\n",
        "        super().__init__()\n",
        "        self.p = p\n",
        "\n",
        "    def forward(self, img):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img (PIL Image or Tensor): Image to be autocontrasted.\n",
        "\n",
        "        Returns:\n",
        "            PIL Image or Tensor: Randomly autocontrasted image.\n",
        "        \"\"\"\n",
        "        if torch.rand(1).item() < self.p:\n",
        "            return F.autocontrast(img)\n",
        "        return img\n",
        "\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '(p={})'.format(self.p)\n",
        "\n",
        "\n",
        "\n",
        "class RandomEqualize(torch.nn.Module):\n",
        "    \"\"\"Equalize the histogram of the given image randomly with a given probability.\n",
        "    If the image is torch Tensor, it is expected\n",
        "    to have [..., 1 or 3, H, W] shape, where ... means an arbitrary number of leading dimensions.\n",
        "    If img is PIL Image, it is expected to be in mode \"P\", \"L\" or \"RGB\".\n",
        "\n",
        "    Args:\n",
        "        p (float): probability of the image being equalized. Default value is 0.5\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, p=0.5):\n",
        "        super().__init__()\n",
        "        self.p = p\n",
        "\n",
        "    def forward(self, img):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img (PIL Image or Tensor): Image to be equalized.\n",
        "\n",
        "        Returns:\n",
        "            PIL Image or Tensor: Randomly equalized image.\n",
        "        \"\"\"\n",
        "        if torch.rand(1).item() < self.p:\n",
        "            return F.equalize(img)\n",
        "        return img\n",
        "\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '(p={})'.format(self.p)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-5352f1f19d2e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0maccimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInterpolationMode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_interpolation_modes_from_int\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aD9Iy5EgvZQ9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}